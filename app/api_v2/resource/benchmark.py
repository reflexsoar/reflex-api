from flask_restx import Resource, Namespace, fields, inputs as xinputs

from ..utils import token_required, user_has
from .shared import ISO8601, mod_user_list

from app.api_v2.model import Q

from app.api_v2.model.benchmark import (
    BenchmarkRule, BenchmarkRuleset, BenchmarkResult
)

api = Namespace('Benchmark', description='Benchmark', path='/benchmark')

mod_benchmark_rule_assess = api.model('BenchmarkRuleAssess', {
    'language': fields.String(required=True, description='The language of the script'),
    'script': fields.String(required=True, description='The script to assess the rule'),
    'args': fields.List(fields.String(required=False, description='The arguments to pass to the script')),
    'success': fields.Integer(required=True, description='The expected success code from the script')
})

mod_benchmark_rule_details = api.model('BenchmarkRuleDetails', {
    'uuid': fields.String(description='The rule UUID'),
    'rule_id': fields.String(description='The rule ID'),
    'organization': fields.String(description='The organization UUID'),
    'name': fields.String(description='The rule name'),
    'description': fields.String(description='A description of why this rule exists and what it does'),
    'version': fields.Integer(description='The rule version'),
    'assess': fields.Nested(mod_benchmark_rule_assess, description='The script to assess the rule'),
    'remediation': fields.Nested(mod_benchmark_rule_assess, description='The script to remediate the rule'),
    'risk_score': fields.Integer(description='A risk score to set on alerts generated by this rule'),
    'severity': fields.Integer(description='The rule severity'),
    'auto_remediate': fields.Boolean(description='Should the rule be automatically remediated?'),
    'category': fields.List(fields.String(description='The rule category')),
    'framework': fields.List(fields.String(description='The rule framework')),
    'platform': fields.List(fields.String(description='The rule platform')),
    'created_at': ISO8601(description='The date and time the rule was created'),
    'updated_at': ISO8601(description='The date and time the rule was last updated'),
    'system_managed': fields.Boolean(description='Is the rule managed by the system?'),
    'created_by': fields.String(description='The user that created the rule'),
    'updated_by': fields.String(description='The user that last updated the rule')
})

mod_benchmark_rule_list = api.model('BenchmarkRuleList', {
    'rules': fields.List(fields.Nested(mod_benchmark_rule_details), description='The List of Rules')
})

mod_benchmark_ruleset_details = api.model('BenchmarkRulesetDetails', {
    'uuid': fields.String(description='The ruleset UUID'),
    'organization': fields.String(description='The organization UUID'),
    'name': fields.String(description='The ruleset name'),
    'rules': fields.List(fields.String(description='The rules in the ruleset')),
    'description': fields.String(description='A description of why this ruleset exists and what it does')
})

mod_benchmark_ruleset_list = api.model('BenchmarkRulesetList', {
    'rulesets': fields.List(fields.Nested(mod_benchmark_ruleset_details), description='The List of Rulesets')
})

mod_benchmark_metric = api.model('BenchmarkMetric', {
    'total': fields.Integer(description='The total number of results for the rule'),
    'passed': fields.Integer(description='The number of results that passed'),
    'failed': fields.Integer(description='The number of results that failed'),
    'error': fields.Integer(description='The number of results that had an error')
})

mod_benchmark_result_metrics = api.model('BenchmarkResultMetrics', {
    'metrics': fields.List(fields.Nested(mod_benchmark_metric), description='The List of Metrics')
})

mod_benchmark_result_create = api.model('BenchmarkResultCreate', {
    'rule_id': fields.String(required=True, description='The rule ID'),
    'rule_uuid': fields.String(description='The rule UUID'),
    'agent': fields.String(description='The agent UUID'),
    'status': fields.String(description='The result status'),
    'output': fields.String(description='The output of the check'),
    'assessed_at': ISO8601(description='The date and time the rule was assessed'),
    'rule_version': fields.Integer(description='The version of the rule')
})

@api.route("/metrics")
class BenchmarkRuleMetrics(Resource):

    @api.doc(security="Bearer")
    #@api.marshal_with(mod_benchmark_result_metrics)
    @api.response(200, 'Success')
    @api.response(401, 'Unauthorized')
    @token_required
    @user_has('view_benchmarks')
    def get(self, current_user):
        '''List Benchmark Results'''
        search = BenchmarkRule.search(skip_org_check=True)

        # Filter for organization is None or organization is the user's organization
        search = search.filter('bool', should=[Q('term', system_managed=True), Q('term', organization=current_user.organization)])

        search = search.filter('term', current=True)

        results = search.scan()

        rule_uuids = [r.uuid for r in results]

        rule_metrics = {}
        
        search = BenchmarkResult.search().filter('terms', rule_uuid=rule_uuids)

        for result in search.scan():
            if result.rule_id not in rule_metrics:
                rule_metrics[result.rule_id] = {
                    'total': 0,
                    'passed': 0,
                    'failed': 0,
                    'error': 0
                }
            
            rule_metrics[result.rule_id]['total'] += 1
            rule_metrics[result.rule_id][result.status] += 1

        return {'metrics': rule_metrics}, 200


@api.route("/rules")
class BenchmarkRuleDetails(Resource):

    @api.doc(security="Bearer")
    @api.marshal_with(mod_benchmark_rule_list)
    @api.response(200, 'Success')
    @api.response(401, 'Unauthorized')
    @token_required
    @user_has('view_benchmarks')
    def get(self, current_user):
        '''List Benchmark Rules'''
        search = BenchmarkRule.search(skip_org_check=True)

        # Filter for organization is None or organization is the user's organization
        search = search.filter('bool', should=[Q('term', system_managed=True), Q('term', organization=current_user.organization)])

        # Only return the current version of a rule
        search = search.filter('term', current=True)

        results = search.scan()

        return {'rules': list(results)}, 200
    

@api.route("/result")
class BenchmarkResultCreate(Resource):

    @api.doc(security="Bearer")
    @api.expect(mod_benchmark_result_create)
    @api.marshal_with(mod_benchmark_result_create)
    @api.response(200, 'Success')
    @api.response(401, 'Unauthorized')
    @token_required
    @user_has('create_benchmark_result')
    def post(self, current_user):
        '''Create Benchmark Result'''
        data = api.payload

        # Check to see if there is already an existing result for this
        # rule, organization, agent, version and status
        search = BenchmarkResult.search()

        search = search.filter('term', rule_id=data['rule_id'])
        search = search.filter('term', agent=data['agent'])
        search = search.filter('term', rule_version=data['rule_version'])

        results = search.execute()

        # If there is an existing result, check to see if the status has changed
        # If the status has changed, create a new result and move the old result
        # to the history index
        if len(results) > 0:
            result = results[0]

            if result.status != data['status']:
                result.create_history_entry()

                result.status = data['status']
                result.output = data['output']
                result.assessed_at = data['assessed_at']
                result.save()

                return result, 200
        
        # If there is no existing result, create a new one
        result = BenchmarkResult(**data)
        result.save()

        return result, 200